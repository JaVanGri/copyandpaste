#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
End-to-end Workflow für die Aufbereitung patientenbezogener Textdaten.
Bei gesetztem SKIP_EXISTING werden Teilschritte übersprungen, wenn die
jeweiligen Ausgabedateien bereits vorhanden sind.
"""

import logging
import sys
import re
from datetime import date
from pathlib import Path
from dateutil.relativedelta import relativedelta
from prompts.tables import TABLES
from utils.rag import retrieve_for_table
import numpy as np
from collections import defaultdict

from utils.llmutils import (
    init_llm,
    read_text_file,
    run_llm_task,
    setup_logging,
    parse_args,
)
from utils.stringdateutils import find_dates, normalize_date, DATE_REGEX, get_overlap_matrix, merge_overlapping_segments

# ------------------------------------------------------------------------------
# Globale Schalter
# ------------------------------------------------------------------------------
SKIP_EXISTING = True          # ← True ⇒ vorhandene Outputs werden wiederverwendet
# ------------------------------------------------------------------------------


def main() -> None:
    """Orchestriert den vollständigen Workflow."""
    setup_logging()
    args = parse_args()

    # CLI-Override, falls parse_args() bereits --skip-existing kennt
    skip_existing = getattr(args, "skip_existing", SKIP_EXISTING)

    base_dir: Path = args.base_dir
    patient_dir: Path = base_dir / args.patient_folder

    if not patient_dir.is_dir():
        logging.error("Patient directory does not exist: %s", patient_dir)
        sys.exit(1)

    # ------------------------------------------------------------------
    # LLM initialisieren
    # ------------------------------------------------------------------
    llm = init_llm(
        model=args.model,
        base_url=args.base_url,
        temperature=args.temperature,
    )

    # ------------------------------------------------------------------
    # Eingabepfade
    # ------------------------------------------------------------------
    path_diagnosis = patient_dir / "Arztbriefe" / "0_diagnoseköpfe"
    path_therapy   = patient_dir / "Arztbriefe" / "1_therapieundverlauf"
    path_befunde_dir = patient_dir / "Befunde"

    # ------------------------------------------------------------------
    # Rohtexte laden
    # ------------------------------------------------------------------
    try:
        diagnosis_str = read_text_file(path_diagnosis)
        therapy_str   = read_text_file(path_therapy)
    except FileNotFoundError as exc:
        logging.error("Aborting: %s", exc)
        sys.exit(1)

    # ------------------------------------------------------------------
    # Schritt 1 – Diagnosis vereinheitlichen
    # ------------------------------------------------------------------
    out_diag_path = patient_dir / "unified_diagnosis.txt"

    diag_prompt = read_text_file(base_dir / "prompts" / "unifi_diagnosis.pmt")
    if skip_existing and out_diag_path.exists():
        logging.info("Skipping diagnosis unification – output exists: %s", out_diag_path)
    else:
        run_llm_task(
            llm=llm,
            prompt_template=diag_prompt,
            sections={"Diagnosis": diagnosis_str},
            output_path=out_diag_path,
            task_name="Unify Diagnosis",
        )

    # ------------------------------------------------------------------
    # Schritt 2 – Therapy vereinheitlichen
    # ------------------------------------------------------------------
    out_therapy_path = patient_dir / "unified_therapy.txt"

    therapy_prompt = read_text_file(base_dir / "prompts" / "unifi_therapy.pmt")
    if skip_existing and out_therapy_path.exists():
        logging.info("Skipping therapy unification – output exists: %s", out_therapy_path)
    else:
        run_llm_task(
            llm=llm,
            prompt_template=therapy_prompt,
            sections={"Therapy": therapy_str},
            output_path=out_therapy_path,
            task_name="Unify Therapy",
        )

    # ------------------------------------------------------------------
    # Schritt 3 – Vereinheitlichte Blöcke einlesen
    # ------------------------------------------------------------------
    try:
        unified_diagnosis_str = read_text_file(out_diag_path)
        unified_therapy_str   = read_text_file(out_therapy_path)
    except FileNotFoundError as exc:
        logging.error("Unified output missing: %s", exc)
        sys.exit(1)

    # ------------------------------------------------------------------
    # Schritt 4 – Events extrahieren
    # ------------------------------------------------------------------
    extracted_events_path = patient_dir / "extracted_events.txt"
    extract_events_prompt = read_text_file(base_dir / "prompts" / "extract_events.pmt")
    if skip_existing and extracted_events_path.exists():
        logging.info("Skipping event extraction – output exists: %s", extracted_events_path)
    else:
        run_llm_task(
            llm=llm,
            prompt_template=extract_events_prompt,
            sections={
                "Diagnosis Head": unified_diagnosis_str,
                "Therapy and Course": unified_therapy_str,
            },
            output_path=extracted_events_path,
            task_name="Extract Events",
        )

    logging.info("LLM-basierte Tasks abgeschlossen.")

    # ------------------------------------------------------------------
    # Schritt 5 – Befunde den Events zuordnen
    # ------------------------------------------------------------------
    try:
        extracted_events_str = read_text_file(extracted_events_path)
    except FileNotFoundError as exc:
        logging.error("Extracted events file not found: %s", exc)
        sys.exit(1)

    event_blocks = extracted_events_str.split("\n\n")[1:]
    events = [block for block in event_blocks if [normalize_date(m.group()) for m in re.finditer(DATE_REGEX, block, re.VERBOSE | re.IGNORECASE) ]]

    # Alle Befund-Dateien einlesen
    befunde = [
        p.read_text(encoding="utf-8", errors="ignore")
        for p in path_befunde_dir.glob("*.txt")
    ]

    # Debug-Ausgabe: erkannte Datumsangaben pro Befund
    for i, text in enumerate(befunde):
        matches = [
            normalize_date(m.group())
            for m in re.finditer(DATE_REGEX, text, re.VERBOSE | re.IGNORECASE)
        ]
        logging.debug("Befund %02d – erkannte Daten: %s", i, matches)

    befund_dates = [find_dates(befund) for befund in befunde]
    event_dates  = [find_dates(event)  for event  in events]

    # (min,max)-Intervalle bilden
    event_date_ranges = [
        (min(dts), max(dts)) if dts else (None, None) for dts in event_dates
    ]

    befund_date_ranges = []
    for b_dates in befund_dates:
        if not b_dates:                     # leere Liste → kein Intervall
            befund_date_ranges.append((None, None))
            continue

        dates_iso = [date.fromisoformat(d) for d in b_dates]
        max_d = max(dates_iso)
        min_d = min(dates_iso)
        lower = max_d - relativedelta(months=4)   # exakt 4 Monate zurück
        if min_d < lower:
            min_d = lower
        befund_date_ranges.append((min_d.isoformat(), max_d.isoformat()))

    overlap_matrix = get_overlap_matrix(event_date_ranges, befund_date_ranges)
    logging.info("Overlap-Matrix (Zeilen=Events, Spalten=Befunde):\n%s", overlap_matrix)

    ###

    tables = args.tables
    print("tables:",tables)
    print(TABLES)
    for table in tables:
        print(table in TABLES)
        if table in TABLES:
            table_columns = TABLES[table]['COLS']
            event_table_befund_dict = {event: [] for event in events}
            print("Schubadubudidida")

            for i, event in enumerate(events):
                print(f"Event {i+1}:")
                # events that overlap with the current event
                list_of_befunde = [ befund for j,befund in enumerate(befunde) if overlap_matrix[i][j] ]
                if not list_of_befunde:
                    print("No overlapping befunds found.")
                    continue
                print(f"  Found {len(list_of_befunde)} overlapping befunds.")
                docs = [ (str(i), befund) for i, befund in enumerate(list_of_befunde) ]
                retrieved_befunde = retrieve_for_table(table_columns, docs, debug=False,use_rerank=False,k=10,dedup_thresh=0.9)
                event_table_befund_dict[event] = retrieved_befunde

            # Ihre ursprüngliche Struktur
            data = event_table_befund_dict
            by_event_befund = defaultdict(dict)
            # Invertieren
            for event, befunds in data.items():
                #print(befunds)
                by_event_befund[event] = defaultdict(list)
                if type(befunds) is dict:
                    for label, befunde in befunds.items():
                        for befund in befunde:
                            befund_id = befund['befund_id']
                            start_token = befund['start_tok']
                            end_token = befund['end_tok']
                            text = befund['text']
                            #print(f"befund_id: {label}: {befund_id}")
                            if befund_id not in by_event_befund[event]:
                                by_event_befund[event][befund_id] = [{
                                    'start_tok': start_token,
                                    'end_tok': end_token,
                                    'text': text,
                                    'label' : label
                                }]
                            else:
                                by_event_befund[event][befund_id].append({
                                    'start_tok': start_token,
                                    'end_tok': end_token,
                                    'text': text,
                                    'label' : label
                                })

            for event, befund_dict in by_event_befund.items():
                for befund_id, segs in befund_dict.items():
                    befund_dict[befund_id] = merge_overlapping_segments(segs)

            final_info_string = ""
            for event in events:
                befunde = by_event_befund[event]
                final_info_string += f"Event: {event}\n"
                final_info_string += "Weitere Informaioenaus Befunden:\n"
                for befund_id, segments in befunde.items():
                    final_info_string += f"-Befund ID: {befund_id}\n"
                    for segment in segments:
                        final_info_string += f"\tText: {segment['text']}, evtl relevant für: {', '.join(segment['labels'])}\n"
                final_info_string += "\n"
            print(final_info_string)
            print("\n------------------\n\n")

            final_info_string = "Information from Documents: \n" + final_info_string

            # Table Prompt vorbereiten
            table_prompt = TABLES[table]['PROMPT']
            # LLM-Aufruf für die Tabelle
            table_output_path = Path(patient_dir / f"{table}_output.json")
            if skip_existing and table_output_path.exists():
                logging.info("Skipping table generation – output exists: %s", table_output_path)
            else:
                run_llm_task(
                    llm=llm,
                    prompt_template=table_prompt,
                    sections={"Documents": final_info_string},
                    output_path=table_output_path,
                    task_name=f"Generate Table {table}",
                )
                logging.info("Tabelle %s erfolgreich generiert und gespeichert: %s", table, table_output_path)
    logging.info("Workflow abgeschlossen.")
        
    


if __name__ == "__main__":
    main()
